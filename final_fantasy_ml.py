# -*- coding: utf-8 -*-
"""Final_fantasy_ml.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dUz1t7li0KftQB_2hv6gGdSrV2UwoI4j
"""

import pandas as pd
import altair as alt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Sequential
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau

games = pd.read_parquet('/content/drive/MyDrive/Colab Data/games.parq')
players = pd.read_parquet('/content/drive/MyDrive/Colab Data/players.parq')
plays = pd.read_parquet('/content/drive/MyDrive/Colab Data/plays.parq')
tackles = pd.read_parquet('/content/drive/MyDrive/Colab Data/tackles.parq')
tracking = pd.read_parquet('/content/drive/MyDrive/Colab Data/tracking_all_weeks.parq')

combine = pd.read_csv('/content/drive/MyDrive/Colab Data/combine_data.csv')
charting = pd.read_csv('/content/drive/MyDrive/Colab Data/ftn_charting.csv')
injuries = pd.read_csv('/content/drive/MyDrive/Colab Data/injuries.csv')
rush = pd.read_csv('/content/drive/MyDrive/Colab Data/rush.csv')
tackle = pd.read_csv('/content/drive/MyDrive/Colab Data/tackle_nfl.csv')
rosters = pd.read_csv('/content/drive/MyDrive/Colab Data/team_rosters.csv')

fantasy = pd.read_csv('/content/drive/MyDrive/Colab Data/2022_fantasy.csv')
fantasy = fantasy.rename(columns={'PPR':'2022_PPR'})
fantasy_results = pd.read_csv('/content/drive/MyDrive/Colab Data/fantasy.csv')
fantasy['Player'] = fantasy['Player'].str.replace('[*+]', '', regex=True)
fantasy_results = fantasy_results[['Player','PPR']]

fantasy = fantasy.merge(fantasy_results, on='Player',how='left')
fantasy.loc[58,'Player'] = 'Kenneth Walker'
fantasy.loc[432,'Player'] = 'Velus Jones'
fantasy.loc[70,'Player'] = 'Brian Robinson'
injuries['practice_primary_injury'] = injuries['practice_primary_injury'].astype('category')
injuries['report_primary_injury'] = injuries['report_primary_injury'].astype('category')
injuries['practice_status'] = injuries['practice_status'].astype('category')
injuries = injuries.drop(columns=['season','game_type','team','week','position','first_name','last_name','report_secondary_injury','practice_secondary_injury','date_modified'])
injuries = pd.get_dummies(injuries,columns=['report_primary_injury','report_status','practice_primary_injury','practice_status'],drop_first=True)
injuries_sum = injuries.groupby(['gsis_id','full_name']).sum().reset_index()
injuries_sum

#positions=['WR','RB','QB','TE']
positions=['WR']

players_filtered = players[players['position'].isin(positions)]
combine_new = combine.rename(columns={'player_name':'displayName'})

players_join = players_filtered.merge(combine_new,on='displayName',how='left')

fantasy_rename = fantasy.rename(columns={'Player':'displayName'})
joined_fantasy = players_join.merge(fantasy_rename,on='displayName',how='left')
joined_fantasy = joined_fantasy.dropna(subset=['PPR'])

#joined_fantasy.sort_values(by=['PPR'],ascending=False)
#players_join

injuries_sum = injuries_sum.rename(columns={'full_name':'displayName'})

joined_injury = joined_fantasy.merge(injuries_sum,on='displayName',how='left')
joined_injury = injuries_sum.merge(joined_fantasy,on='displayName',how='left')
joined_injury = joined_injury.dropna(subset=['PPR'])

joined_injury[['feet','inch']] = joined_injury['ht'].str.split('-', expand=True)
joined_injury['feet'] = joined_injury['feet'].fillna(0).astype(int)
joined_injury['inch'] = joined_injury['inch'].fillna(0).astype(int)
joined_injury['inches'] = joined_injury['feet']*12 + joined_injury['inch']

joined_injury = joined_injury.drop(columns=['gsis_id','-9999','ht','feet','inch','school','draft_team','pfr_id','cfb_id','pos','FantPos','birthDate'])

joined_injury = pd.get_dummies(joined_injury, columns=['collegeName','position','Tm'])

joined_injury.sort_values(by=['PPR'],ascending=False)

#string_columns = joined_injury.select_dtypes(include=['datetime']).dtypes
#print(string_columns)

X = joined_injury.drop(columns=['displayName','PPR'])

y = joined_injury['PPR']

scaler=MinMaxScaler()

X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=42)

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
X_val = scaler.transform(X_val)

X_train = np.nan_to_num(X_train)
X_val = np.nan_to_num(X_val)
X_test = np.nan_to_num(X_test)
y_train = np.nan_to_num(y_train)
y_val = np.nan_to_num(y_val)
y_test = np.nan_to_num(y_test)

opt = keras.optimizers.Adam()

# Build the model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(256, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model with the specified optimizer
model.compile(loss="mean_squared_error", optimizer=opt, metrics=['mse'])

# Define callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=100,restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001,restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=150, batch_size=32, callbacks=[early_stop])
#history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32)

# Make predictions
predictions = np.round(model.predict(X_val), 1)

# Convert the training history to a DataFrame
hist = pd.DataFrame(history.history)

hist = hist.reset_index()

import matplotlib.pyplot as plt
def plot_history():
    plt.figure()
    plt.xlabel('Epoch')
    plt.ylabel('Mean Square Error ')
    plt.plot(hist['index'], hist['mse'], label='Train Error')
    plt.plot(hist['index'], hist['val_mse'], label = 'Val Error')
    plt.legend()
    plt.ylim([0,8000])

plot_history()

result = mean_squared_error(y_val, predictions, squared=False)
result

rsquared = r2_score(y_val,predictions)
rsquared

predictions_test = np.round(model.predict(X_test),1)
test_result = mean_squared_error(y_test, predictions_test, squared=False)
test_result

test_squared = r2_score(y_test, predictions_test)
test_squared

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score
from tensorflow.keras.models import load_model, Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import pickle

class PlayerEnvironment:
    def __init__(self, data):
        self.data = data
        self.features = ['PassAtt','Yds','PassTD','RushAtt', 'RushYds', 'RushTD', 'Tgt', 'Rec', 'RecYds', 'RecTD','TotTD']
        self.state = self.data[self.features].values  # State is defined by selected features
        self.max_steps = len(self.data)  # Number of steps in the environment
        self.current_step = 0

    def reset(self):
        self.current_step = 0
        return self.state[self.current_step]

    def step(self, action):
        # Assume action is the index of the feature to explore
        reward = self.data['PPR'].iloc[self.current_step]  # Reward is based on PPR score
        self.current_step += 1
        done = self.current_step >= self.max_steps
        next_state = self.state[self.current_step] if not done else None
        return next_state, reward, done

class RLAgent:
    def __init__(self, state_size, action_size):
        self.state_size = state_size
        self.action_size = action_size
        self.model = self._build_model()

    def _build_model(self):
        model = Sequential([
            Dense(128, input_dim=self.state_size, activation='relu'),
            Dense(256, activation='relu'),
            Dense(64, activation='relu'),
            Dense(self.action_size, activation='linear')
        ])
        model.compile(loss='mse', optimizer=Adam())
        return model

    def act(self, state):
        return np.argmax(self.model.predict(state.reshape(1, -1)))  # Simple policy: maximize predicted reward

    def train(self, state, reward):
        target = np.array(reward)  # Example: use reward as target (simplified)
        self.model.fit(state.reshape(1, -1), target.reshape(1, -1), epochs=1, verbose=0)

data = joined_injury.copy()
train_data = data.sample(frac=0.8, random_state=42)
val_data = data.drop(train_data.index)

# Create environments for training and validation
env = PlayerEnvironment(joined_injury)
train_env = PlayerEnvironment(train_data)
val_env = PlayerEnvironment(val_data)

# Define RL agent
state_size = len(env.features)
action_size = len(env.features)
agent = RLAgent(state_size, action_size)

num_episodes = 200

cumulative_rewards = []
val_cumulative_rewards = []

actual_ppr = []
predicted_ppr = []
val_actual_ppr = []
val_predicted_ppr = []

for episode in range(num_episodes):
    state = env.reset()
    total_reward = 0.0
    done = False

    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)

        # Train RL agent based on reward (PPR score)
        agent.train(state, reward)

        actual_ppr.append(reward)
        predicted_ppr.append(agent.model.predict(state.reshape(1, -1))[0][0])

        state = next_state
        total_reward += reward

    cumulative_rewards.append(total_reward)

    # Validation
    val_state = val_env.reset()
    val_total_reward = 0.0
    val_done = False

    while not val_done:
        val_action = agent.act(val_state)
        val_next_state, val_reward, val_done = val_env.step(val_action)

        val_actual_ppr.append(val_reward)
        val_predicted_ppr.append(agent.model.predict(val_state.reshape(1, -1))[0][0])

        val_state = val_next_state
        val_total_reward += val_reward

    val_cumulative_rewards.append(val_total_reward)

    print(f"Episode {episode + 1} - Total Reward: {total_reward}")

train_rmse = np.sqrt(mean_squared_error(actual_ppr, predicted_ppr))
train_r2 = r2_score(actual_ppr, predicted_ppr)
val_rmse = np.sqrt(mean_squared_error(val_actual_ppr, val_predicted_ppr))
val_r2 = r2_score(val_actual_ppr, val_predicted_ppr)
avg_cumulative_reward = np.mean(cumulative_rewards)

df_prep = {'actual_train':actual_ppr,'predicted_train':predicted_ppr}
val_prep = {'actual_val':val_actual_ppr,'predicted_val':val_predicted_ppr}

rl_df = pd.DataFrame(data=df_prep)
val_df = pd.DataFrame(data=val_prep)
rl_df['predicted_train'] = rl_df['predicted_train'].astype(float)
rl_df['actual_train'] = rl_df['actual_train'].astype(float)
val_df['predicted_val'] = val_df['predicted_val'].astype(float)
val_df['actual_val'] = val_df['actual_val'].astype(float)

rl_df['squared_error_train'] = (rl_df['predicted_train'] - rl_df['actual_train'])**2
val_df['squared_error_val'] = (val_df['predicted_val'] - val_df['actual_val'])**2

rl_df['r_squared_train'] = r2_score(rl_df['actual_train'], rl_df['predicted_train'])
val_df['r_squared_val'] = r2_score(val_df['actual_val'], val_df['predicted_val'])

# RMSE for each row (not typical)
rl_df['rmse_train'] = np.sqrt(rl_df['squared_error_train'])
val_df['rmse_val'] = np.sqrt(val_df['squared_error_val'])
rl_df['episode'] = np.arange(len(rl_df)) // 104 + 1
val_df['episode'] = np.arange(len(val_df)) // 21 + 1

episode_rmse = rl_df.groupby('episode')[['rmse_train','r_squared_train']].mean().reset_index()
episode_rmse_val = val_df.groupby('episode')[['rmse_val','r_squared_val']].mean().reset_index()

rl_df['r_squared_train'] = r2_score(rl_df['actual_train'], rl_df['predicted_train'])
val_df['r_squared_val'] = r2_score(val_df['actual_val'], val_df['predicted_val'])

t_rmse_l = episode_rmse['rmse_train'].tolist()
v_rmse_l = episode_rmse_val['rmse_val'].tolist()
t_rs = episode_rmse['r_squared_train'].to_list()
v_rs = episode_rmse_val['r_squared_val'].to_list()

print(f"Training RMSE: {t_rmse_l[-1]}, Validation RMSE: {v_rmse_l[-1]}")
print(f"Training R-squared (R²): {t_rs[-1]}, Validation R-squared (R²): {v_rs[-1]}")
print(f"Average Cumulative Reward over {num_episodes} episodes: {avg_cumulative_reward}")

def plot_rl():
    plt.figure()
    plt.xlabel('Episode')
    plt.ylabel('Root Mean Square Error ')
    plt.plot(episode_rmse['episode'],episode_rmse['rmse_train'], label='Training RMSE')
    plt.plot(episode_rmse_val['episode'],episode_rmse_val['rmse_val'], label='Validation RMSE')
    plt.legend()

plot_rl()

#def plot_rl():
#    episodes = np.arange(1, num_episodes + 1)
#    plt.figure()
#    plt.xlabel('Episode')
#    plt.ylabel('Cumulative Reward')
#    plt.plot(episodes, cumulative_rewards, label='Training Cumulative Reward')
#    plt.plot(episodes, val_cumulative_rewards, label='Validation Cumulative Reward')
#    plt.legend()
#    plt.show()

#plot_rl()

# Uncomment to save new model and agent
import pickle
from tensorflow.keras.models import load_model

#agent.model.save('/content/drive/MyDrive/ml_models/rl_agent_rec.h5')

#with open('/content/drive/MyDrive/ml_models/rl_agent_rec.pkl', 'wb') as file:
#    pickle.dump(agent, file)

loaded_model = load_model('/content/drive/MyDrive/ml_models/rl_agent_rec.h5')

with open('/content/drive/MyDrive/ml_models/rl_agent_rec.pkl','rb') as file:
  loaded_agent = pickle.load(file)

loaded_agent.model = loaded_model

def predict_new_data(new_data, agent):
    new_state = new_data[env.features].values
    predicted_ppr = agent.model.predict(new_state)
    return predicted_ppr

#new_data = pd.read_csv('') # Load the data you want to predict here

new_data = pd.DataFrame({
    'PassAtt': [0, 25],
    'Yds': [0, 200],
    'PassTD': [0, 1],
    'RushAtt': [6, 15],
    'RushYds': [15, 80],
    'RushTD': [0, 1],
    'Tgt': [171, 10],
    'Rec': [119, 7],
    'RecYds': [1799, 100],
    'RecTD': [13, 2],
    'TotTD': [13, 4]
})


predicted_ppr_new_data = predict_new_data(new_data, loaded_agent)
print(predicted_ppr_new_data)

