# -*- coding: utf-8 -*-
"""Final copy of starter_bikes.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IgnojRD6MQI0W8sUhzFh-ZpWhNhCJpID
"""

import pandas as pd
import altair as alt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, Sequential
from sklearn.metrics import confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, median_absolute_error
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.preprocessing import MinMaxScaler
from sklearn import preprocessing
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import ReduceLROnPlateau

bikes_og = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')
bikes_og.head()

bikes = bikes_og
bikes['target'] = bikes['casual'] + bikes['registered']
bikes = bikes.drop(columns = ['casual', 'registered'])

X = bikes.drop(columns=['target'])
y = bikes['target']

X['dteday'] = pd.to_datetime(X['dteday'])
X['day'] = X['dteday'].dt.day.astype('category')
X['month'] = X['dteday'].dt.month.astype('category')
X['year'] = X['dteday'].dt.year.astype('category')

X['season'] = np.where(X['season']==1,'winter',np.where(X['season']==2,'spring',
                        np.where(X['season']==3,'summer','fall')))
X['season'] = X['season'].astype('category')
X = pd.get_dummies(X, drop_first=True)
X['dteday'] = X['dteday'].astype('datetime64[s]').astype(int)

# fit scaler on training data

X.head()

minmax_scaler = MinMaxScaler()
X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=42)

X_train = minmax_scaler.fit_transform(X_train) # fit the scale to the training data
X_test = minmax_scaler.transform(X_test)  # fit the scale to the training data
X_val = minmax_scaler.transform(X_val)

#Sigmoid: Good for binary classification problems, but can suffer from vanishing gradient issues.
#ReLU (Rectified Linear Unit): Commonly used in hidden layers due to its ability to mitigate vanishing gradient problems.
#Linear: Typically used in the output layer for regression tasks.

# Learning Rate Schedule

# Early Stopping

# Patience

from keras.optimizers import SGD


# Define the SGD optimizer
opt = keras.optimizers.Adam()

# Build the model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(256, activation='relu'))
model.add(Dense(64, activation='relu'))
model.add(Dense(1, activation='linear'))

# Compile the model with the specified optimizer
model.compile(loss="mean_squared_error", optimizer=opt, metrics=['mse'])

# Define callbacks
early_stop = EarlyStopping(monitor='val_loss', patience=100,restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32, callbacks=[early_stop, reduce_lr])
#history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=500, batch_size=32)

# Make predictions
predictions = np.round(model.predict(X_val), 1)

# Convert the training history to a DataFrame
hist = pd.DataFrame(history.history)

hist = hist.reset_index()

import matplotlib.pyplot as plt
def plot_history():
    plt.figure()
    plt.xlabel('Epoch')
    plt.ylabel('Mean Square Error ')
    plt.plot(hist['index'], hist['mse'], label='Train Error')
    plt.plot(hist['index'], hist['val_mse'], label = 'Val Error')
    plt.legend()
    # plt.ylim([0,50])

plot_history()

result = mean_squared_error(y_val, predictions, squared=False)
result

#No Stop: 87.225
#With Stop: 85.100

rsquared = r2_score(y_val,predictions)
rsquared
#No Stop: 0.93488
#With Stop: 0.93801

predictions_test = np.round(model.predict(X_test),1)
test_result = mean_squared_error(y_test, predictions_test, squared=False)
test_result

#80.94291807347575

test_squared = r2_score(y_test, predictions_test)
test_squared

#0.945297530311598

month_chart = pd.DataFrame(X_test)
month_chart['prediction'] = list(predictions_test.flatten())
month_chart['actual'] = list(y_test)
month_chart['error'] = np.sqrt((month_chart['actual'] - month_chart['prediction'])**2)
month_chart['dteday'] = month_chart[0]
month_chart = month_chart[['dteday','prediction','actual','error']]

#st = bikes.drop(columns=['target','dteday'])
#date = st['dteday'].to_list()
#st['date'] = pd.to_datetime(st['date'])
#st = pd.get_dummies(st, drop_first=True)
#st['date'] = st['date'].astype('datetime64[s]').astype(int)
#st.head()

#st = minmax_scaler.transform(st)

#st = pd.DataFrame(st)

#st['date'] = date

#st['dteday'] = st[1]

#st = st[['dteday','date']]

#st.head()
#month_chart.head()

#month_chart = month_chart.merge(st,on='dteday',how='left')

month_chart.head()
import altair as alt
alt.data_transformers.disable_max_rows()

alt.Chart(month_chart,title='How off were our predictions?').mark_boxplot(size=100).encode(
   y='error'
).properties(
    width=200,
    height=400
)

bikes = bikes_og
bikes['target'] = bikes['casual'] + bikes['registered']
bikes = bikes.drop(columns = ['casual', 'registered'])

X = bikes.drop(columns=['target'])
y = bikes['target']

X['dteday'] = pd.to_datetime(X['dteday'])
X = pd.get_dummies(X, drop_first=True)
X['dteday'] = X['dteday'].astype('datetime64[s]').astype(int)

minmax_scaler = MinMaxScaler()
X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=42)

X_train = minmax_scaler.fit_transform(X_train) # fit the scale to the training data
X_test = minmax_scaler.transform(X_test)  # fit the scale to the training data
X_val = minmax_scaler.transform(X_val)

test_holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini.csv')
test_holdout_answers = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/biking_holdout_test_mini_answers.csv')

test_holdout_answers['target'] = test_holdout_answers['casual'] + test_holdout_answers['registered']
test_holdout_answers = test_holdout_answers.drop(columns = ['casual', 'registered'])

X_holdout = test_holdout
y_holdout = test_holdout_answers

X_holdout['dteday'] = pd.to_datetime(X_holdout['dteday'])
X_holdout = pd.get_dummies(X_holdout, drop_first=True)
X_holdout['dteday'] = X_holdout['dteday'].astype('datetime64[s]').astype(int)
X_holdout = minmax_scaler.transform(X_holdout)
test_holdout_answers.head()

holdout_predictions = np.round(model.predict(X_holdout),1)

holdout_result = mean_squared_error(y_holdout, holdout_predictions, squared=False)
holdout_result

holdout_squared = r2_score(y_holdout,holdout_predictions)
holdout_squared

final_df = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')

Xf = final_df
Xf['dteday'] = pd.to_datetime(Xf['dteday'])

Xf['day'] = Xf['dteday'].dt.day.astype('category')
Xf['season_spring'] = False
Xf['season_summer'] = False
Xf['season'] = np.where(Xf['season']==1,'winter','fall')
Xf['season'] = Xf['season'].astype('category')

Xf = pd.get_dummies(Xf, drop_first=True)
Xf['dteday'] = Xf['dteday'].astype('datetime64[s]').astype(int)

Xf['month_2'] = False
Xf['month_3'] = False
Xf['month_4'] = False
Xf['month_5'] = False
Xf['month_6'] = False
Xf['month_7'] = False
Xf['month_8'] = False
Xf['month_9'] = False
Xf['month_10'] = False
Xf['month_11'] = False
Xf['month_12'] = True

Xf['year_2012'] = False
Xf['year_2013'] = False
Xf['year_2014'] = False
Xf['year_2015'] = False
Xf['year_2016'] = False
Xf['year_2017'] = False
Xf['year_2018'] = False
Xf['year_2019'] = False
Xf['year_2020'] = False
Xf['year_2021'] = False
Xf['year_2022'] = False
Xf['year_2023'] = True

final_df['dteday'].unique()
Xf.info()

Xf = minmax_scaler.transform(Xf)

final_predictions = np.round(model.predict(Xf), 1)
final_df = pd.DataFrame(final_predictions)
final_df.to_csv('team1-module4-predictions(feature engineering).csv')